% 114 words - old
% Large language models (LLMs) can automate test case generation, but the prompts needed to reach coverage inflate tokens and API costs. We address this cost inflation by comparing two prompt-engineering strategies: (i) Docstring removal, (ii) LLMLingua-2 compression. The techniques were benchmarked on Python projects with three LLMs. LLMLingua-2 reduced API costs by 6.8\% without compromising the code coverage, which remained at 100\%. Docstrings removal cut cost for one model but increased it for the others, revealing model-specific dependencies on documentation quality. These results demonstrate that careful prompt-engineering can deliver predictable, low-risk savings, advancing the deployment of economical LLM-based test case generation.


% 120 words
Large language models (LLMs) automate test case generation but require lengthy prompts to reach 100\% code coverage, increasing token usage and API costs. We compare two prompt-engineering strategies: removing docstrings and compressing prompts with LLMLingua-2. Across Python projects using Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and GPT-4.1. LLMLingua-2 reduce API costs by 6.8\% while keeping 100\% code coverage. Docstring removal achieved savings only on GPT-4.1. For Claude, the coefficient of variation was 19.5\% with docstrings and 32.7\% without. This is indicating more stable performance when docstrings is retained. Therefore, these findings reveal that the need for docstrings depends on the model and demonstrate that prompt-engineering can deliver predictable cost saving, advancing the deployment of economical LLM-based test case generation.

