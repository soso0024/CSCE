The core idea of the proposed method is to reduce input token counts in LLM-based test case generation while preserving generation quality. This study investigates three strategies:

\begin{enumerate}
    \item \textbf{Removal of in-code documentation of functions} (referred to as Docstrings in this study)
\vspace{0.2cm}
    \item \textbf{Natural language compression using LLMLingua-2}: applied to both prompts and docstrings, to retain meaningful information while minimizing token count.
\end{enumerate}

\subsection{Role of LLMs in Test Case Generation}
LLMs are trained on extensive corpora of source code and natural language, enabling them to autonomously generate test code when provided with function specifications or intended behaviors. Given such prompts, LLMs can produce test cases that account for various edge cases and invalid inputs for function arguments, and even generate mocking logic for external API calls when necessary.

During this process, LLMs interpret the prompt—which includes instructions and code-related context—to infer the most appropriate and coverage-effective test scenarios. As a result, the quality and coverage of test cases generated by LLMs depend heavily on the content of the prompt. When the prompt includes sufficient context and clear guidance, LLMs can output test cases that closely resemble those written by human developers. Conversely, if the prompt lacks clarity or contains irrelevant or misleading information, the generated code may fail to compile or include redundant or ineffective test logic.

Therefore, prompt-engineering plays a critical role in maximizing the capabilities of LLMs and ensuring the generation of high-quality test cases.

\subsection{LLMs and AI Agent}
In this study, we adopt Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, GPT-4.1 as LLMs. This decision is based on the two key considerations:

\begin{enumerate}
    \item All three models are offered by the top three vendors listed on Galileo's Agent Leaderboard on Hugging Face \cite{agent_leaderboard}. We selected the top-performing model from each vendor.
\vspace{0.2cm}
    \item To examine whether the results of the proposed prompt- engineering methods—such as docstrings removal and natural language compression across different LLMs.
\end{enumerate}

The dataset used in this study, detailed later, includes functions with frequent cross-module calls, making it challenging for any LLMs to generate correct test code in a single prompt. Post-generation correction and retries are inevitable. As such, an automated framework capable of iterative test generation and error correction is essential.

To meet this need, we incorporate Cline, an open-source AI agent that facilitates autonomous iterative generation and correction. Cline offers the following key features:

\begin{itemize}[label={$\bullet$}]
    \item Plan Mode and Act Mode architecture, in which requirements and files are analyzed in Plan Mode and executed step-by-step in Act Mode based on a shared plan.
\vspace{0.2cm}
    \item Model-agnostic design, supporting multiple LLMs and enabling context windows exceeding 200k tokens. This allows for comprehensive analysis of entire codebases or multiple modules simultaneously.
\end{itemize}


\subsection{Prompt-Engineering}
Prompt-engineering refers to the practice of optimizing the input messages given to LLMs including system instructions, user queries, and contextual information to elicit desired and effective outputs \cite{llmlingua}. In the context of this study, the primary challenge of prompt-engineering is to selectively curate and concisely present the necessary information about the target code to LLMs, without including redundant or irrelevant details.

Traditionally, one might simply feed the entire source code and related documentation into the prompt and instruct LLMs to "generate test code for the given function." However, as discussed in the previous section, such prompts tend to become excessively verbose, leading to inefficiencies in both cost and performance. To address this, our study investigates the effect of reducing non-essential content in prompts—specifically, by removing docstrings.

Docstrings are in-code natural language comments that describe what a function or class does and how it should be used. While potentially informative, they may not always contribute directly to generating effective test cases as I described in Section 2. Thus represent an opportunity for prompt reduction.

The base prompt used in this study is known as .clinerules, showed in Prompt 1. The .clinerules prompt is a type of custom instruction automatically inserted as a system prompt at the beginning of each Plan/Act step in the Cline framework. It defines coding standards, constraints, and recommended procedures to guide the LLM’s behavior.

\input{chapters/92_base_prompt}

In addition, all prompts are formatted using Markdown. Previous research has shown that presenting prompts in Markdown can improve LLMs response accuracy compared to plain-text formatting \cite{he2024doespromptformattingimpact}.

\subsection{Natural Language Compression}

To reduce prompt length, this study employs LLMLingua-2, a model designed for natural language compression \cite{pan2024llmlingua2datadistillationefficient}. LLMLingua-2 uses a token classifier to iteratively remove low-importance words, allowing it to achieve high compression rates while preserving semantic content. Previous research has shown that at a compression rate of 0.9 corresponding to a 5–10\% reduction in total tokens the performance drop is less than one percentage point. Based on this evidence, we adopt the same compression rate (0.9) in this study.

\subsubsection{Base Model}
The encoder used for LLMLingua-2 is mbert-base. Given that docstrings typically consist of a few lines per code and follow standardized formats describing the purpose, parameters, and return values of functions, their lexical and syntactic variation is limited. As such, complex language phenomena requiring large-scale models are not expected to be present, making mbert-base a suitable and lightweight choice for this compression task.

In addition, because docstrings are short by nature, we did not specify any mandatory tokens to preserve in LLMLingua-2. Specifically, the Tokens\_to\_Preserve parameter was set to an empty set (Ø), and the option to force preservation of tokens containing numeric values was disabled.

\subsubsection{Parameter Settings}
The LLMLingua-2 compression was applied using the following parameters:

\begin{itemize}[label={$\bullet$}]
    \item Base Model: mbert-base
\vspace{0.2cm}
    \item Tokens to Preserve: Ø
\vspace{0.2cm}
    \item Compression Rate: 0.9
\end{itemize}

Prompt 2 shows the result of compressing the original base prompt in Prompt 1.

\input{chapters/93_compressed_prompt}

Table \ref{tab:token_reduction_llmlingua2} presents the number of tokens after applying LLMLingua-2 compression, along with the reduction rate compared to the base prompt. With the compression rate set to 0.9, the observed token reduction was approximately 11.2\%. Token counts were obtained using the official endpoint provided by Anthropic, the developer of Claude \footnote{\url{https://docs.anthropic.com/en/api/messages-count-tokens}}.

\begin{table}[h!]
  \centering
  \caption{Token Reduction by LLMLingua-2}
  \label{tab:token_reduction_llmlingua2}
  \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Condition} & \textbf{Token Counts} & \textbf{Reduction Rate} \\
    \hline
    Prompt 1: Before Compression & 392 & -- \\
    Prompt 2: After Compression  & 348 & -11.2\% \\
    \hline
  \end{tabular}
\end{table}
