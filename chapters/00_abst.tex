% 114 words
Large language models (LLMs) can automate test case generation, but the prompts needed to reach coverage inflate tokens and API costs. We address this cost barrier by comparing two prompt-engineering strategies: (i) Docstring removal, (ii) LLMLingua-2 compression. The techniques were benchmarked on Python projects with three LLMs. LLMLingua-2 reduced API costs by 6.8\% without compromising the code coverage, which remained at 100\%. Docstrings removal cut cost for one model but increased it for the others, revealing model-specific dependencies on documentation quality. These results demonstrate that careful prompt-engineering can deliver predictable, low-risk savings, advancing the deployment of economical LLM-based test case generation.
