This study confirmed that prompt-engineering can meaningfully reduce cost of LLM-based automated test case generation while maintaining perfect effectiveness.

\begin{enumerate}
    \item \textbf{Docstrings removal} reduced tokens and cost for GPT-4.1 (–17\% tokens, –12.8\% cost) but raised both metrics for Claude 3.7 Sonnet and Gemini 2.5 Pro Preview, revealing model-specific dependence on in-code documentation.
    \vspace{0.3cm}
    
    \item \textbf{LLMLingua-2 compression} of prompts and docstrings delivered the most consistent savings—up to a 6.8\% cost drop for Claude 3.7 Sonnet—with negligible quality impact, making it a low-risk default optimisation.
    \vspace{0.3cm}
    
    \item \textbf{Docstrings inclusion} notably enhanced code comprehension and output quality for Claude 3.7 Sonnet, leading to lower total token usage, reduced API costs, and fewer correction requests when compared to prompts without docstrings.
\end{enumerate}

% volatile: 不安定な

These findings demonstrate that prompt-size control, particularly via lightweight compression, can make large-scale automated test case generation economically sustainable. Practitioners should start with compression, assess each model’s sensitivity to documentation, and escalate to more aggressive reductions only when coverage and stability stay intact.
