Most existing LLM-based test generation approaches still rely on incorporating extensive context—entire source files, function definitions, specifications, comments, and historical coverage data—into their prompts in pursuit of higher coverage. This inevitably exacerbates tokens and API cost.

In this research, we focus on the problem of prompt inflation in LLM-based test case generation. We propose a new method that leverages prompt-engineering to reduce input tokens while preserving generation quality, thereby reducing the overall number of tokens and API usage cost without sacrificing test effectiveness.
